/*
 * Copyright (c) 2014, Imagination Technologies LLC and Imagination
 * Technologies Limited.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted under the terms of the MIPS Free To Use 1.0
 * license that you will have received with this package. If you haven't
 * received this file, please contact Imagination Technologies or see the
 * following URL for details.
 * http://codescape-mips-sdk.imgtec.com/license/IMG-free-to-use-on-MIPS-license
 *
 */


/*
 * m32cache_ops.sx: MIPS32 cache support functions
 */


/*
 * Note:	 pessimistic hazard timings assumed.
 */

#include "m32cache.h"

#if 1 /*#cache(m32)*/

#ifndef CSMIPS_CACHE_EXTRAROUTINES

/*
 * void m32_flush_cache (void)
 *
 * Writeback and invalidate all caches
 */
LEAF(m32_flush_cache)
	SIZE_CACHE(a1,mips_dcache_size)

	/* writeback and invalidate primary caches individually */
	lw	a2,mips_dcache_linesize
	li	a0,KSEG0_BASE
	cacheop(a0,a1,a2,Index_Writeback_Inv_D)

9:	lw	a1,mips_icache_size
	lw	a2,mips_icache_linesize
	blez	a1,9f
	li	a0,KSEG0_BASE
	cacheop(a0,a1,a2,Index_Invalidate_I)

9:	lw	a1,mips_scache_size
	lw	a2,mips_scache_linesize
	blez	a1,9f
	sync
	li	a0,KSEG0_BASE
	cacheop(a0,a1,a2,Index_Writeback_Inv_S)

9:	sync
	jr.hb	ra
END(m32_flush_cache)

/*
 * void m32_flush_dcache (void)
 *
 * Writeback and invalidate data caches only
 */
LEAF(m32_flush_dcache)
	SIZE_CACHE(a1,mips_dcache_size)

	/* writeback and invalidate primary data cache */
	lw	a2,mips_dcache_linesize
	li	a0,KSEG0_BASE
	cacheop(a0,a1,a2,Index_Writeback_Inv_D)

9:	lw	a1,mips_scache_size
	lw	a2,mips_scache_linesize
	blez	a1,9f
	sync
	li	a0,KSEG0_BASE
	cacheop(a0,a1,a2,Index_Writeback_Inv_S)

9:	sync
	jr.hb	ra
END(m32_flush_dcache)

/*
 * void m32_flush_icache (void)
 *
 * Writeback and invalidate instruction cache only
 */
LEAF(m32_flush_icache)
	SIZE_CACHE(a1,mips_icache_size)

	/* writeback and invalidate primary instruction cache */
	lw	a2,mips_icache_linesize
	li	a0,KSEG0_BASE
	cacheop(a0,a1,a2,Index_Invalidate_I)

9:	lw	a1,mips_scache_size
	blez	a1,9f
	lw	a2,mips_scache_linesize
	li	a0,KSEG0_BASE
	cacheop(a0,a1,a2,Index_Writeback_Inv_S)

9:	sync
	jr.hb	ra
END(m32_flush_icache)

/*
 * void m32_clean_cache (unsigned kva, size_t n)
 *
 * Writeback and invalidate address range in all caches
 */
LEAF(m32_clean_cache)
#if __mips_isa_rev < 2
XLEAF(m32_sync_icache)
#endif
	SIZE_CACHE(a2,mips_dcache_linesize)
	vcacheop(a0,a1,a2,Hit_Writeback_Inv_D)

9:	lw	a2,mips_icache_linesize
	blez	a2,9f
	vcacheop(a0,a1,a2,Hit_Invalidate_I)

9:	lw	a2,mips_scache_linesize
	blez	a2,9f

	sync
	vcacheop(a0,a1,a2,Hit_Writeback_Inv_S)

9:	sync
	jr.hb	ra
END(m32_clean_cache)

#if __mips_isa_rev >= 2
/*
 * void m32_sync_icache (unsigned kva, size_t n)
 *
 * Synchronise icache and dcache for virtual address range
 */
LEAF(m32_sync_icache)
	/* check for bad size */
	addu	maxaddr,a0,a1
	blez	a1,9f

	/* get synci step and skip if not required */
	rdhwr	a2,$1
	addu	maxaddr,-1
	beqz	a2,9f

	/* ensure stores complete */
	sync

	/* align to line boundaries */
	subu	mask,a2,1
	not	mask
	and	addr,a0,mask
	and	maxaddr,mask

	/* the cacheop loop */
10: 	synci	0(addr)
	beq     addr,maxaddr,9f
	addu   	addr,a2
	b 10b

9:	sync
	jr.hb	ra
END(m32_sync_icache)
#endif


/*
 * void m32_clean_dcache (unsigned kva, size_t n)
 *
 * Writeback and invalidate address range in data caches
 */
LEAF(m32_clean_dcache)
	SIZE_CACHE(a2,mips_dcache_linesize)
	vcacheop(a0,a1,a2,Hit_Writeback_Inv_D)

9:	lw	a2,mips_scache_linesize
	blez	a2,9f
	sync
	vcacheop(a0,a1,a2,Hit_Writeback_Inv_S)

9:	sync
	jr.hb	ra
END(m32_clean_dcache)

/*
 * void m32_clean_dcache_nowrite (unsigned kva, size_t n)
 *
 * Invalidate (but don't writeback) address range in data caches
 * XXX Only safe if region is totally cache-line aligned.
 */
LEAF(m32_clean_dcache_nowrite)
	SIZE_CACHE(a2,mips_dcache_linesize)
	vcacheop(a0,a1,a2,Hit_Invalidate_D)

9:	lw	a2,mips_scache_linesize
	blez	a2,9f
	vcacheop(a0,a1,a2,Hit_Invalidate_S)

9:	sync
	jr.hb	ra
END(m32_clean_dcache_nowrite)




/*
 * Cache locking
 *
 * The MIPS32 cache architecture does support per-line cache locking.
 *
 * WARNING: if you lock any cache lines, then don't call the
 * mips_flush_xcache routines, because these will flush the
 * locked data out of the cache too; use only mips_clean_xcache.
 */


/*
 * void m32_lock_dcache (void *data, size_t n)
 *
 * Load and lock a block of date into the d-cache
 */
LEAF(m32_lock_dcache)
	SIZE_CACHE(a2,mips_dcache_linesize)
	vcacheop(a0,a1,a2,Fetch_Lock_D)
	sync
9:	jr.hb	ra
END(m32_lock_dcache)


/*
 * void m32_lock_icache (void *code, size_t n)
 *
 * Load and lock a block of instructions into the i-cache
 */
LEAF(m32_lock_icache)
	SIZE_CACHE(a2,mips_icache_linesize)
	vcacheop(a0,a1,a2,Fetch_Lock_I)
	sync
9:	jr.hb	ra
END(m32_lock_icache)


LEAF(m32_lock_scache)
	j	ra
END(m32_lock_scache)

#endif /* !CSMIPS_CACHE_EXTRAROUTINES */




/*
 * The following functions operate on individual cache levels, or use
 * indexed addressing, so they are probably only useful for cache
 * diagnostics or possibly virtual memory operating systems.
 */

#if defined(CSMIPS_CACHE_EXTRAROUTINES) || defined(ITROM)

/*
 * void m32_flush_cache_nowrite (void *base)
 *
 * Invalidate but don't writeback all caches (probably only
 * sensible for cache diagnostics). Use "base" as the
 * cacheable base address, in case kseg0 is uncacheable.
 */
LEAF(m32_flush_cache_nowrite)
	/* disable all i/u and cache exceptions */
	mfc0	a3,$sr
	li	a1,~SR_IE
	and	a1,a3
	or	a1,SR_ERL		# will this work?
	mtc0	a1,$sr
#if __mips_isa_rev < 2
	ssnop; ssnop
#endif
	ehb

	mtc0	zero,$errctl
	mtc0	zero,$taglo		# 4K taglo / 20Kc itaglo
	mtc0	zero,$taghi		# 4K taghi / 20Kc itaghi
	mtc0	zero,$taglo,2		# 20Kc dtaglo
	mtc0	zero,$taghi,2		# 20Kc dtaghi
	mtc0	zero,$taglo,4		# 25Kf L23taglo
	mtc0	zero,$taghi,4		# 25Kf L23taghi
	ehb

	/* invalidate primary caches individually */
	lw	a1,mips_dcache_size
	lw	a2,mips_dcache_linesize
	blez	a1,9f
	cacheop(a0,a1,a2,Index_Store_Tag_D)

9:	lw	a1,mips_icache_size
	lw	a2,mips_icache_linesize
	blez	a1,9f
	cacheop(a0,a1,a2,Index_Store_Tag_I)

9:	sync
	lw	a1,mips_scache_size
	lw	a2,mips_scache_linesize
	blez	a1,9f
	cacheop(a0,a1,a2,Index_Store_Tag_S)

9:	mtc0	a3,$sr
#if __mips_isa_rev < 2
	ssnop; ssnop
#endif
	sync
	jr.hb	ra
END(m32_flush_cache_nowrite)


/*
 * void m32_clean_dcache_indexed (unsigned kva, size_t n)
 *
 * Writeback and invalidate indexed range in primary data cache
 */
LEAF(m32_clean_dcache_indexed)
	SIZE_CACHE(a3,mips_dcache_size)
	lw	v0,mips_dcache_ways
	lw	a2,mips_dcache_linesize
	divu	a3,v0			# calculate set size

1:	icacheop(a0,a1,a2,a3,Index_Writeback_Inv_D)
	addu	a0,a3			# do next set
	subu	v0,1			# until all done
	bnez	v0,1b

	sync

9:	jr.hb	ra
END(m32_clean_dcache_indexed)


/*
 * void m32_clean_icache (unsigned kva, size_t n)
 *
 * Writeback and invalidate address range in instruction caches
 */
LEAF(m32_clean_icache_nowrite)
	SIZE_CACHE(a2,mips_icache_linesize)
	vcacheop(a0,a1,a2,Hit_Invalidate_I)

	/* flush secondary cache */
9:	lw	a2,mips_scache_linesize
	blez	a2,9f
	vcacheop(a0,a1,a2,Hit_Invalidate_S)

9:	jr.hb	ra
END(m32_clean_icache_nowrite)


/*
 * void m32_clean_icache_indexed (unsigned kva, size_t n)
 *
 * Writeback and invalidate indexed range in primary instruction cache
 */
LEAF(m32_clean_icache_indexed)
	SIZE_CACHE(a3,mips_icache_size)
	lw	v0,mips_icache_ways
	lw	a2,mips_icache_linesize
	divu	a3,v0			# calculate set size

1:	icacheop(a0,a1,a2,a3,Index_Invalidate_I)
	addu	a0,a3			# do next set
	subu	v0,1			# until all sets done
	bnez	v0,1b

9:	jr.hb	ra
END(m32_clean_icache_indexed)

/*
 * void m32_clean_scache (unsigned kva, size_t n)
 *
 * Writeback and invalidate address range in secondary cache
 */
LEAF(m32_clean_scache)
	/* flush secondary cache */
	SIZE_CACHE(a2,mips_scache_linesize)
	vcacheop(a0,a1,a2,Hit_Writeback_Inv_S)
	sync
9:	jr.hb	ra
END(m32_clean_scache)


/*
 * void m32_clean_scache_nowrite (unsigned kva, size_t n)
 *
 * Invalidate an address range in secondary cache
 */
LEAF(m32_clean_scache_nowrite)
	/* flush secondary cache */
	SIZE_CACHE(a2,mips_scache_linesize)
	vcacheop(a0,a1,a2,Hit_Invalidate_S)
9:	jr.hb	ra
END(m32_clean_scache_nowrite)

/*
 * void mips_clean_sdcache_nowrite (unsigned kva, size_t n)
 *
 * Invalidate an address range in secondary data cache
 */
LEAF(m32_clean_sdcache_nowrite)
9:	j	ra
END(m32_clean_sdcache_nowrite)

/*
 * void mips_clean_sicache_nowrite (unsigned kva, size_t n)
 *
 * Invalidate an address range in secondary instruction cache
 */
LEAF(m32_clean_sicache_nowrite)
9:	j	ra
END(m32_clean_sicache_nowrite)


/*
 * void m32_hit_writeback_inv_dcache (unsigned kva, size_t n)
 *
 * Writeback and invalidate address range in primary data cache
 */
LEAF(m32_hit_writeback_inv_dcache)
	SIZE_CACHE(a2,mips_dcache_linesize)
	vcacheop(a0,a1,a2,Hit_Writeback_Inv_D)
	sync
9:	jr.hb	ra
END(m32_hit_writeback_inv_dcache)

/*
 * void m32_hit_writeback_inv_scache (unsigned kva, size_t n)
 *
 * Writeback and invalidate address range in secondary cache
 */
LEAF(m32_hit_writeback_inv_scache)
	SIZE_CACHE(a2,mips_dcache_linesize)
	vcacheop(a0,a1,a2,Hit_Writeback_Inv_S)
	sync
9:	jr.hb	ra
END(m32_hit_writeback_inv_scache)

#endif /* CSMIPS_CACHE_EXTRAROUTINES */

#endif /* #cache(m32) */
